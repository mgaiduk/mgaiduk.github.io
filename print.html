<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Tensorflow for recsys</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body>
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="chapter_1.html"><strong aria-hidden="true">1.</strong> Chapter 1. What is this book about</a></li><li class="chapter-item expanded "><a href="chapter_2.html"><strong aria-hidden="true">2.</strong> Chapter 2. First glance at the data</a></li><li class="chapter-item expanded "><a href="chapter_3.html"><strong aria-hidden="true">3.</strong> Chapter 3. Proper input pipeline and the model</a></li><li class="chapter-item expanded "><a href="chapter_4.html"><strong aria-hidden="true">4.</strong> Chapter 4. The model</a></li><li class="chapter-item expanded "><a href="chapter_5.html"><strong aria-hidden="true">5.</strong> Chapter 5. Tidying up</a></li><li class="chapter-item expanded "><a href="chapter_6.html"><strong aria-hidden="true">6.</strong> Chapter 6. Training on TPU</a></li><li class="chapter-item expanded "><a href="chapter_7.html"><strong aria-hidden="true">7.</strong> Chapter 7. LazyAdam, or some hacks for training speed</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Tensorflow for recsys</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="chapter-1-what-is-this-book-about"><a class="header" href="#chapter-1-what-is-this-book-about">Chapter 1. What is this book about</a></h1>
<p>Who am I and why am I writing this</p>
<hr />
<p>This book is about Tensorflow and recsys.</p>
<p>It has been a couple of years since leaving a big tech company (Yandex) specialized in search and recommendations. Back then, we had an entire ecosystem for everything that we could need: mapreduce, SQL, machine learning, research reproducibility and process automation (airflow/flyte). Right now, I tend to think that what we had there was actually pretty good, better then stuff that is available in the open source or as enterprise solutions (like Google Cloud).</p>
<p>Back then, we had a c++ written tool to train neural networks. It was fast, well-integrated into our ecosystem - it was easy to set up distributed training while loading data from MapReduce table, for example. It was not flexible - but we did implement most important stuff into it: dense networks, embedding tables, transformers, convolution networks.</p>
<p>But all that is in the past. Now I have to leave in the open world, use open technologies, learn their weaknessess and strengths. It is at least my third time trying to set up big scale training in Tensorflow. For me it was always a huge pain in the butt.</p>
<p>Tensorflow tutorials usually deal with datasets that fit entirely into memory, sometimes even just preloaded with a provided library functions. When you deal with real-world problems, you have to first parse the data somehow, and Python is not really effective at that.</p>
<p>It is really easy to make mistakes - and Python is not really good at pointing where exactly those mistakes were made.for example, I was getting this error: <code>tensorflow ValueError: 'outputs' must be defined before the loop.</code>. My code had no loops and it was not obvious what 'outputs' should mean. The fix was to set <code>steps_per_execution=1</code> in <code>model.compile()</code>. Thank god this error was popular enough and Google knew about it. Getting data from wherever it is stored into model training inputs was always troublesome - parsing it with my own code in Python was too costly, library functions were not exactly what I needed for my input, and so on. Sometimes stuff works but is just too slow - like EmbeddingLayer when you try to scale it up to millions of parameters. </p>
<p>Another big problem was to set up training in a convenient, reproducible way. To make models accept neat configs and parameters so that different ML engineers can just reuse the same code - this seems to be a weak point in a lot of organizations, with people copy-pasting each other code, storing it locally or on some VMs without commiting it, rewriting the same thing over and over again. To save models in proper format, usable from another language during inference, with meta information about model parameters, input data and architecture. </p>
<p>Now, after all that struggle, I got into a team where some of these problems are solved. We train models with around 100m vocab size, 50b training pool. We store data in google cloud (bigquery + gcs) and train models on TPUs. We focus on recsys models (think &quot;Netflix Challenge&quot;). So here is what the book is about:</p>
<ul>
<li>How to get data from bigquery to gcs, to tensorflow tensors in a simple and performant way</li>
<li>Scaling the data: datasets that don't fit into memory</li>
<li>How to set up model training, how to train on TPUs, what TPUs are, why can they be better and are they really better?</li>
<li>Convenience features setup for model customization and saving</li>
<li>Recsys architecture overview and experiments: collaborative filtering models, deep neural networks, transformers and other things to try</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="chapter-2-first-glance-at-the-data"><a class="header" href="#chapter-2-first-glance-at-the-data">Chapter 2. First glance at the data</a></h1>
<p>To make the code reproducible, we will be using public dataset. I've decided to go with Movielens (https://grouplens.org/datasets/movielens/), although it is not available in bigquery. We will upload it to bq by hand.</p>
<h2 id="getting-movielens-data"><a class="header" href="#getting-movielens-data">Getting movielens data</a></h2>
<p>At the time of writing, movielens data is available to download at https://grouplens.org/datasets/movielens/.<br />
Inside the zip archive, there is a file called ratings.csv with the following content:</p>
<pre><code>userId,movieId,rating,timestamp
1,296,5.0,1147880044
...
</code></pre>
<p>Which is just what we needed! Now, let's load it to bq.<br />
First, we need to load it to gcs:</p>
<pre><code>gsutil cp ratings.csv &quot;gs://mgaiduk/tmp/ratings&quot;
</code></pre>
<p>Then, we upload it to bq:</p>
<pre><code>bq load \
--autodetect \
--source_format=CSV \
mgaiduk.ratings \
&quot;gs://mgaiduk/tmp/ratings&quot;
</code></pre>
<p>Finally, do a quick select to validate that the data is there and to see how it looks like:</p>
<pre><code>-- in bigquery console
SELECT * FROM `mgaiduk.ratings` LIMIT 1000
</code></pre>
<p>This is the result:
|Row|userId|movideId|rating|timestamp|
|---|------|--------|------|---------|
|1  |    70|    3948|2.0   |1255219128|
|2  |    188|    653|2.0   |1025333400|
|3  |    243|    103249|2.0   |1464280162|
Preparation is done, and now the data looks like what you'd expect in a big tech company residing in GCP: a bigquery table. Now let's talk about how to actually load it into Tensorflow</p>
<h2 id="bq-to-tensorflow"><a class="header" href="#bq-to-tensorflow">BQ to tensorflow?</a></h2>
<p>All operations in Tensorflow are performed on Tensors. A tensor has to reside in memory. There are 2 challenges for that.</p>
<p>First of all, python code is very slow. Tensorflow intrinsics are written in something else (C?). For user pipelines, they provide a <code>@tf.function</code> decorator and other facilities that do approximately the following: they look into your python code, turn it into some intrinsic, optimized representation, and compile it to make it really fast. The only problem is - it works only for code working with tensors. But you have to get your data and turn it into tensors somehow!<br />
Second problem is scaling up. Entire dataset will not fit in memory or on disk on a single machine. We will need to get it from some scalable storage and parse on-the-fly. We will want our models training on many servers, either on several TPU pods or several GPU workers. Therefore, we need to write the code in such a way that each worker will load its own portion of the dataset.</p>
<p>In the past, I've tried different approaches of doing that: loading an entire file into memory, then turning into tensors in one go; parsing input line by line in python code and passing it to Tensorflow through a &quot;dataset from generator&quot; feature. Neither seemed to be effective or convenient - &quot;dataset from generator&quot; api is exceptionally unintuitive, and you have to call a lot of python code in a cycle before converting data to tensors, which makes it slow and ineffectve. One solution that I find both easy and effective is to use either <a href="https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset">tf.data.TFRecordDataset</a> or <a href="https://www.tensorflow.org/api_docs/python/tf/data/TextLineDataset">tf.data.TextLineDataset</a> that allow you to create a dataset (thus solving the initial &quot;bootstrap&quot; problem of getting your data into tensors somehow!) using filename or even a file pattern for local filesystem or GCS. GCS one is excatly what we need for future scalable solutions!</p>
<p>Tensorflow Datasets API provides only 2 file formats for easy data loading: TextLineDataset and TFRecordDataset. Other solutions (like parsing Parquet files) seem to also be available, but they are less popular, sometimes have experimental APIs, and, at a first glance, are not made as convenient as the first two to use in Tensorflow. So we will stick to those two in this book. </p>
<h3 id="getting-data-from-bq-to-gcs---csv"><a class="header" href="#getting-data-from-bq-to-gcs---csv">Getting data from BQ to GCS - CSV</a></h3>
<p>Let's see how we can get them from BQ to gcs.</p>
<p>Native extraction API, available through &quot;export&quot; tab in BQ UI, or <code>bq extract</code> command line tool, has following formats: CSV, NEWLINE_DELIMITED_JSON, AVRO, PARQUET. Avro and Parquet create binary files that could not be parsed from Tensorflow with the dataset API functions that we agreed on. And there is no TFRecord here - we will have to resort to Dataflow Templates to do that ) But that is to be expected - TFRecord is a low-popularity, very specific format, and its not like Tensorflow and Bigquery were made by the same company.</p>
<p>We are left with CSV and JSON. CSV is actually pretty good, I've seen it used at large scale in production. Its downside is that it is a text format, which will bloat the size of numbers. It is a &quot;plain&quot; format, which won't work out of the box for arrays or nested structures. Upside is that it doesn't store anything extra, like labels per each record (json and tfrecord both do that). It is also human-readable, so it will be possible to check the data out with your own eyes.<br />
JSON is a bit more bloated, with labels per each row; but it makes it possible to store nested structures and arrays.</p>
<p>Here is how to export data from bq to gcs:</p>
<pre><code>bq extract --noprint_header --destination_format CSV --compression GZIP mgaiduk.ratings &quot;gs://mgaiduk-us-central1/ratings/csv_gzip/part*&quot;
</code></pre>
<p>There are some interesting details here. First of all, here we are using &quot;native&quot; export utility, which has access to private datastore api not available to general public. It is faster and cheaper then the alternatives (like dataflow), takes mere minutes for petabyte datasets. However, using this way limits us to format options described above - CSV or JSON.</p>
<p>Exported files represent a sharded gcs file: </p>
<pre><code>part000000000000	12.1 MB	application/octet-stream	Mar 3, 2023, 1:19:15 PM	Standard	Mar 3, 2023, 1:19:15 PM	Not public	—	
Google-managed key
—			
part000000000001	12.1 MB	application/octet-stream	Mar 3, 2023, 1:19:14 PM	Standard	Mar 3, 2023, 1:19:14 PM	Not public	—	
Google-managed key
—			
part000000000002	12.1 MB	application/octet-stream	Mar 3, 2023, 1:19:13 PM	Standard	Mar 3, 2023, 1:19:13 PM	Not public	—	
Google-managed key
—	
</code></pre>
<p>Which makes it quite convenient to work with, especially in ditributed training setups.</p>
<p>We do not extract headers. Having to skip 1 line from each file is, again, not quite convenient in distributed training setups.</p>
<p>We use GZIP compression, which is optional, of course. It is supported natively by Tensorflow, and helps partially mitigate the bloat we discussed above. It should be a trade-off between network and storage cost and CPU spent on decompression; whether or not it actually helps speed up the training will be clear from later chapters, when we will run some experiments.</p>
<p>Finally, as can be seen from the bucket name, it is located in a specific region - <code>mgaiduk-us-central1</code>. It is important to track data locality, and to launch tranining in the same region where the data is located. It helps both with input pipeline throughput and network costs.</p>
<h3 id="bq-to-gcs-tfrecords-using-dataflow"><a class="header" href="#bq-to-gcs-tfrecords-using-dataflow">BQ to GCS TFRecords using dataflow</a></h3>
<p>Another alternative to native BQ export is to use Dataflow. The idea is simple: launch a bunch of dataflow workers that will grab data from Bigquery, convert to a desired format, and write it to GCS.</p>
<p>Unlike native export, this doesn't allow workers to use private storage API, so the export will be much slower. There is also a HUGE overhead in time and costs for launching the dataflow job: you have to wait for all the workers to be created and start doing their job; you will not receive any feedback in case something goes wrong, and will have to wait for all the workers to start up, do some retries, and then fail. Typically, on a petabyte of data, the job takes a few hours, and if there are errors in the configuration, the job will fail within half an hour. You also have to pay for worker cpu and other excess resources, which makes it much more costly then native BQ export.<br />
The actual export format here can be anything we want, but Google provides a set of templates with some of the most popular formats already suported, including TFRecords: https://cloud.google.com/dataflow/docs/guides/templates/provided-batch.<br />
I launch it from the UI, because I just couldn't understand how to write an SQL query in a parameter list in command line and not have it complain about syntax. Sadly, Google does not provide any examples on how to do that.</p>
<p>Another big problem with this approach is that it is error-prone. If one of the fields is nullable in the table, and your input data pipeline doesn't expect that - you will spend an entire day (and quite a few hundred bucks!) collecting the data only to learn that you have to fix the problem and do it all over again. If your table format is not supported by the template (say, you have nested structures), your job will run for half an hour before failing. So my advice here is - first test everything, including model training in the final setup - on a sample of data, say, first 100k rows, then collect the entire thing.</p>
<p>TFRecordDataset and TFRecords were designed specifically for Tensorflow. It is a protobuf-based, binary format, which means that there is no bloat from textual representation, and parsing is faster. However, TFRecords save labels for every row, as well as some protobuf metadata. For our example - userId,movieId,rating,timestamp - this makes TFRecord dataset be actually bigger in size then CSV. We shall see if easier parsing makes it worthwile.</p>
<h2 id="look-at-the-data"><a class="header" href="#look-at-the-data">Look at the data</a></h2>
<p>The data is now in GCS, where it will be stored for all our training needs. Now let's have a first glance at the data and on how to actually load it into tensorflow:</p>
<pre><code>import tensorflow as tf
tf.__version__
# csv version
dataset = tf.data.TextLineDataset([&quot;gs://mgaiduk-us-central1/ratings/csv_gzip/part000000000000&quot;], compression_type=&quot;GZIP&quot;)
for line in dataset:
    break
print(line)
</code></pre>
<p><code>tf.Tensor(b'70,3948,2,1255219128', shape=(), dtype=string)</code></p>
<p>As promiseed, parsing the data is very easy. We do the parsing in native Tensorflow function, so no slow python for loops. Output is the dataset with 1 row yielding one csv string tensor. We will need to parse it further to actually use it in our model; but it is already a tensor, so every parsing done on it will be optimized with Tensorflow utilities.</p>
<pre><code># tfrecord version
dataset2 = tf.data.TFRecordDataset([&quot;gs://mgaiduk-us-central1/ratings/tfrecord/train/output-00000-of-00012.tfrecord&quot;])
for line in dataset2:
    break
print(line)
</code></pre>
<p><code>tf.Tensor(b'\n:\n\x11\n\x06userId\x12\x07\x1a\x05\n\x03\xf3\x9d\x03\n\x11\n\x07movieId\x12\x06\x1a\x04\n\x02\xca\x15\n\x12\n\x06rating\x12\x08\x12\x06\n\x04\x00\x00\x80@', shape=(), dtype=string)</code></p>
<p>Code examples are also available as <a href="https://github.com/mgaiduk/mgaiduk.github.io/blob/main/my-first-book/src/code/chapter2/data_first_glance.ipynb">jupyter notebook</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="chapter-3-proper-input-pipeline-and-the-model"><a class="header" href="#chapter-3-proper-input-pipeline-and-the-model">Chapter 3. Proper input pipeline and the model</a></h1>
<p>In the last chapter, we discussed various data storing formats a little bit, and got to take a first look at the data, still not parsed, but already represented as a tensor.<br />
In this chapter, we will build a proper data pipeline, reading data from all our files, parsing it and properly handling sharding during distributed training.</p>
<h3 id="csv-data-parsing"><a class="header" href="#csv-data-parsing">CSV data parsing</a></h3>
<p>In the last chapter, we had roughly the following code:</p>
<pre><code>dataset = tf.data.TextLineDataset([&quot;gs://mgaiduk-us-central1/ratings/csv_gzip/part000000000000&quot;], compression_type=&quot;GZIP&quot;)
for line in dataset.batch(16): # batching added to demonstrate that parsing works on higher dimensional tensors
    break
print(line)
</code></pre>
<p>That reads the data from just one file out of several for our exported table, and does no parsing.</p>
<p>Here is how we can parse it:</p>
<pre><code>defaults = [tf.constant(0, dtype=tf.int64),
           tf.constant(0, dtype=tf.int64),
           tf.constant(0, dtype=tf.float32),
           tf.constant(0, dtype=tf.int64)]
csv_row = tf.io.decode_csv(line, defaults)
csv_row
</code></pre>
<p><code>[&lt;tf.Tensor: shape=(16,), dtype=int64, numpy=array([  70,  188,  243,  359,  426,  440,  446,  626,  634,  700,  734,1044, 1332, 1367, 1409, 1459])&gt;,</code><br />
<code>&lt;tf.Tensor: shape=(16,), dtype=int64, numpy=array([  3948,    653, 103249,   3578,   1500,   3022,   2861,   2162,1006,  46578,   3203,   2406,   1198,   1037,   5530,   1183])&gt;,</code><br />
<code>&lt;tf.Tensor: shape=(16,), dtype=float32, numpy=array([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],dtype=float32)&gt;,</code><br />
<code>&lt;tf.Tensor: shape=(16,), dtype=int64, numpy=array([1255219128, 1025333400, 1464280162,  974700907, 1371502543,1231472902, 1017900083, 1002897585,  865372001, 1350795469,992217420,  944907698, 1529895670, 1185835950, 1288302165,889019866])&gt;]</code></p>
<p>We pass <code>defaults</code> to it to signify which data types are expected, and to use in case an entire column is missing (though it is rather hard to imagine in CSV)</p>
<p>To be able to just call <code>model.fit(training_dataset)</code> on our data, we need to turn it into a dataset, and have it output a tuple with 2 values: features and labels. It is also common to represent features as a dictionary, to know what column means what, and to be able to connect it to model code. Here is the code that does that:</p>
<pre><code>@tf.function
def decode_fn(csv_line):
    defaults = [tf.constant(0, dtype=tf.int64),
           tf.constant(0, dtype=tf.int64),
           tf.constant(0, dtype=tf.float32),
           tf.constant(0, dtype=tf.int64)]
    csv_row = tf.io.decode_csv(csv_line, defaults)
    features = {}
    features[&quot;userId&quot;] = csv_row[0]
    features[&quot;movieId&quot;] = csv_row[1]
    labels = {
        &quot;label&quot;: csv_row[2]
    }
    return (features, labels)
dataset = tf.data.TextLineDataset([&quot;gs://mgaiduk-us-central1/ratings/csv_gzip/part000000000000&quot;],
                                  compression_type=&quot;GZIP&quot;).batch(16, drop_remainder=True).map(decode_fn)
for elem in dataset:
    break
elem # take a look at the resulting data
</code></pre>
<p><code>dataset.map()</code> function takes a dataset and applies a function to each row.<br />
We do the batching first, and then do the parsing. This is because we want to spend less time in our own, python code, and more time in optimized tensorflow intrinsics. Because of this, it is better to turn small tensors into big, high-dimensional tensors first, to have our python code benefit from vectorization.</p>
<p>Another interesting detail is the works of @tf.function. It does roughly the following: upon calling the function for the first time, TF engine sees tensor inputs with concrete shapes, and compiles the code into an execution graph, treating all non-tensor inputs as contants. If non-tensor inputs are not constant, say, you have some training-dependable variable in the pipeline, graph compilation will happen every time, which will probably be slower then just running the python code. All side effects are also executed on graph compile time only. This means that you can insert print statements in the desired places in your code, and see what exactly is being passed around, but only on the first step of your execution.</p>
<p>If tensor shapes change during execution, the training will probably crush (at least on TPU). This is why we have `.batch(16, drop_remainder=True). Normally, when batching limited size dataset, last batch is slightly smaller. We don't want our model to crush right at the end, so we just drop that remainder.</p>
<p>Next, we need to read all the files for our dataset, not just the one file. We could do it like this:</p>
<pre><code>filenames = tf.data.Dataset.list_files(&quot;gs://mgaiduk-us-central1/ratings/csv_gzip/part*&quot;, shuffle=True, seed=42)
dataset = tf.data.TextLineDataset(filenames,
                                  compression_type=&quot;GZIP&quot;).batch(16, drop_remainder=True).map(decode_fn)
for elem in dataset:
    break
elem
</code></pre>
<p><code>TextLineDataset</code> api is smart enough to accept a list of files, or even a dataset that outputs filenames. The only reason NOT to do it like that is proper sharding, which should be done like this instead:</p>
<pre><code>ctx = None # will not be none in distributed strategy, see later
def make_dataset_fn(path):
    dataset = tf.data.TextLineDataset([path], compression_type=&quot;GZIP&quot;)
    dataset = dataset\
        .batch(16, drop_remainder=True)\
    .map(decode_fn)
    return dataset
filenames = tf.data.Dataset.list_files(&quot;gs://mgaiduk-us-central1/ratings/csv_gzip/part*&quot;, shuffle=True, seed=42)
if ctx and ctx.num_input_pipelines &gt; 1:
    filenames = filenames.shard(ctx.num_input_pipelines, ctx.input_pipeline_id)
dataset = filenames.interleave(make_dataset_fn)
for elem in dataset:
    break
elem
</code></pre>
<p><code>tf.data.Dataset.list_files</code> creates a dataset with all filenames matching a pattern.<br />
When we train under a distributed strategy, we will receive a ctx with information about total number of shards and current shard index. Under data parallelism methodology, we want each shard to read its own portion of data. It can be achieved by <code>tf.data.Dataset.shard()</code> function, that takes total number of batches and shard idx as input. It works like this: it reads the dataset and skips all the data except 1/nths. We don't want to actually read and parse all that skipped data; that is why we call this method on filenames dataset.</p>
<p>Finally, we want to add some dataset stuff to make sure input pipeline is not a bottleneck:</p>
<pre><code>ctx = None # will not be none in distributed strategy, see later
filenames = tf.data.Dataset.list_files(&quot;gs://mgaiduk-us-central1/ratings/csv_gzip/part*&quot;, shuffle=True, seed=42)
if ctx and ctx.num_input_pipelines &gt; 1:
    filenames = filenames.shard(ctx.num_input_pipelines, ctx.input_pipeline_id)
dataset = filenames.interleave(make_dataset_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE, deterministic=False, cycle_length=8)
dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)

for elem in dataset:
    break
elem
</code></pre>
<p>New things here:</p>
<ol>
<li>
<p>We call <code>prefetch</code> dataset method. Without it, one iteration of training loop will call dataset processing code to fetch 1 element, then perform a model training step, then call dataset function and so on. If you are doing your training on a multi-core machine, all those extra cores will probably just do nothing during dataset processing step. Having prefetch allows you to process dataset and do model training at the same time. AUTOTUNE parameter usually works, but if you are really interested in trying out different values - you can put some number here.<br />
Important to note that we do <code>prefetch</code> in the very end, making sure we prefetch fully parsed, batched data.</p>
</li>
<li>
<p>dataset.interleave got 2 new parameters: num_parallel_calls and cycle_length. They both are required for parallelism because of how parallel computing works in <code>interleave</code> calls. This is the second crucial step in making sure your input pipeline is not a bottleneck. If your parsing is done on just one core, having prefetch will not be enough to fetch new data with enough speed. Parallel dataset execution makes sure that you give enough cores for the task</p>
</li>
<li>
<p><code>deterministic=False</code> in <code>interleave</code>. In theory, this might speed up input data pipeline, because it allows dataset executor to output results from multiple parallel calls as soon as they are ready. However, if you have prefetch and parallel calls, you probably won't notice latency differences. Important thing is, if you are doing distributed training, to have sharding done BEFORE any non-determinism in the pipeline, otherwise the shards will train on random, partially intersecting parts of data. </p>
</li>
</ol>
<p>Code for this chapter is available as a <a href="https://github.com/mgaiduk/mgaiduk.github.io/blob/main/my-first-book/src/code/chapter3/proper_dataset_parsing.ipynb">jupyter notebook</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="chapter-4-the-model"><a class="header" href="#chapter-4-the-model">Chapter 4. The model</a></h1>
<p>Now that we have our data pipeline, let's build the model. It will be a classical collaborative filtering model, with a layer looking up unique userId's and movieId's into N-dim embeddings, then dot-product, activation and that's it. To do that, we first need to map unique ids to continuous integers from 0 to vocab_size.</p>
<p>There are 2 main ways to do that. One way is to collect a &quot;dictionary&quot;, mapping tokens to integers based on their frequency, and mapping all tokens that are rare enough into a special &quot;out of vocabulary&quot; OOV token. This can be done in Tensorflow directly, though a more scalable solution usually is to do it as a preprocessing step in Bigquery/MapReduce.<br />
The second way is to take hash(id) % vocab_size. Hash itself, as well as modulo operator, will inevitably lead to collisions, which cross-contaminates the signal between different entities and possibly reducing the quality. Hashing is, however, much easier to set up, especially in an &quot;incremental training&quot; setting. We will do some benchmarks later to determine which way is better in respect to quality.</p>
<p>First, here is the code from previous chapter to read the dataset:</p>
<pre><code>import tensorflow as tf
print(tf.__version__)
import tensorflow_recommenders as tfrs
print(tfrs.__version__)

import math
@tf.function
def decode_fn(csv_line):
    defaults = [tf.constant(0, dtype=tf.int64),
           tf.constant(0, dtype=tf.int64),
           tf.constant(0, dtype=tf.float32),
           tf.constant(0, dtype=tf.int64)]
    csv_row = tf.io.decode_csv(csv_line, defaults)
    features = {}
    features[&quot;userId&quot;] = csv_row[0]
    features[&quot;movieId&quot;] = csv_row[1]
    labels = {
        &quot;label&quot;: csv_row[2]
    }
    return (features, labels)
ctx = None # will not be none in distributed strategy, see later
def make_dataset_fn(path):
    dataset = tf.data.TextLineDataset([path], compression_type=&quot;GZIP&quot;)
    dataset = dataset\
        .batch(16, drop_remainder=True)\
    .map(decode_fn)
    return dataset
filenames = tf.data.Dataset.list_files(&quot;gs://mgaiduk-us-central1/ratings/csv_gzip/part*&quot;, shuffle=True, seed=42)
if ctx and ctx.num_input_pipelines &gt; 1:
    filenames = filenames.shard(ctx.num_input_pipelines, ctx.input_pipeline_id)
dataset = filenames.interleave(make_dataset_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE, deterministic=False, cycle_length=8)
dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)
for elem in dataset:
    break
elem
</code></pre>
<p>Here is how we can hash the input:</p>
<pre><code>vocab_size = 1000
hashing_layer = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=vocab_size)
hashing_layer(elem[0][&quot;userId&quot;]) # it's good that we have an eager tensor on our hands, so we can test inputs-outputs to all parts of our model, heh?
</code></pre>
<p>Having hashing layer instead of, say, hashing preprocessing step is good because we can have vocab_size information baked into our model. Different models can have different vocab_size, at which point it becomes necessary to track that information and synchronize it on the preprocessing step of the inference side, which is usually done in a language other than python and tensorflow. Having hashing as just an another layer is a handy way to avoid that.<br />
<code>vocab_size</code> is a hyper-parameter that, ideally, needs to be tuned. Ideally, it should be about the same size as the number of unique IDs. However, entity id's histogram is usually very skewed: with just a few most popular ids getting most of the attention, and &quot;long tail&quot; of rare ids (about 50% of total mass, typically) having just one or a few interactions associated with them. In our CF model, embedding tables will be the most expensive part. 25 millions of unique entities converted to embeddings of 32 4-byte floating point numbers is already 3 gb of data, that will need to be stored in-memory during training and inference time. So this parameter deserves some experimenting to better approximate the trade-offs.</p>
<p>Now we need an embedding layer. There are 2 TF layers that do that that I am aware of: <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding">tf.keras.layers.Embedding</a> and <a href="https://www.tensorflow.org/recommenders/api_docs/python/tfrs/layers/embedding/TPUEmbedding">tfrs.layers.embedding.TPUEmbedding</a>. First one can be used on CPU and GPU. Second one can be used on CPU and TPU. APIs are different, unfortunately. I trained my models on cpu and tpu, so we are sticking with TPUEmbedding. Luckily, cpu quality and performance of both of these layers is identical. Here is how to use it:</p>
<pre><code>embedding_dim = 8
lr = 0.01
initializer = tf.initializers.TruncatedNormal(
    mean=0.0, stddev=1 / math.sqrt(embedding_dim)
)
embedding_layer_feature_config = {
    &quot;userId&quot;: tf.tpu.experimental.embedding.FeatureConfig(
        table=tf.tpu.experimental.embedding.TableConfig(
        vocabulary_size=vocab_size,
        initializer=initializer,
        dim=embedding_dim)),
    &quot;movieId&quot;: tf.tpu.experimental.embedding.FeatureConfig(
        table=tf.tpu.experimental.embedding.TableConfig(
        vocabulary_size=vocab_size,
        initializer=initializer,
        dim=embedding_dim)),
}
optimizer = tf.keras.optimizers.legacy.Adam(learning_rate = lr)
embedding_layer = tfrs.layers.embedding.TPUEmbedding(
    feature_config=embedding_layer_feature_config,
    optimizer=optimizer)
hashed_tensor = {
    &quot;userId&quot;: hashing_layer(elem[0][&quot;userId&quot;]),
    &quot;movieId&quot;: hashing_layer(elem[0][&quot;movieId&quot;])
}
embeddings = embedding_layer(hashed_tensor)
embeddings
</code></pre>
<p>Note here that we only need one embedding layer for all our inputs. It gets a dictionary {name -&gt; tensor} as input, and provides one as an output. Output tensors have shape (batch_size, embedding_dim). Also note that vocab size in the embedding table is the same as we used for hashing layer.<br />
Now we just need to do dot product. The API looks as follows:</p>
<pre><code>tf.keras.backend.batch_dot(user_emb, movie_emb)
</code></pre>
<p>If user_emb and movie_emb are vectors of the shape (batch_size, emb_dim), <code>batch_dot</code> computes per-element dot product.<br />
We also want to have &quot;bias&quot; for users and movies. For example, for imdb movies, &quot;8.7 out of 10&quot; rating can be represented as a movie bias - unpersonalized, overall level of popularity of the movie. The part of the embedding that goes into dot product - personalozed signal, that has no meaning without corresponding user embedding. I found out the hard way that without bias, the model will hardly be able to train, so this is a crucial peace of information. Here is the code to compute dot product + bias:</p>
<pre><code>user_emb = embeddings[&quot;userId&quot;]
movie_emb = embeddings[&quot;movieId&quot;]
user_final_emb = tf.slice(user_emb, begin=[0, 0], size=[user_emb.shape[0],  user_emb.shape[1] - 1])
user_final_bias = tf.slice(user_emb, begin=[0, user_emb.shape[1] - 1], size=[user_emb.shape[0],  1])
movie_final_emb = tf.slice(movie_emb, begin=[0, 0], size=[movie_emb.shape[0],  movie_emb.shape[1] - 1])
movie_final_bias = tf.slice(movie_emb, begin=[0, movie_emb.shape[1] - 1], size=[movie_emb.shape[0],  1])
out = tf.keras.backend.batch_dot(user_final_emb, movie_final_emb) + user_final_bias + movie_final_bias
out
</code></pre>
<p>We say that last element of the embedding is treated as bias (since it is just another user/movie dependant number, same as embedding vector components), while all except last - go into dot product. This is represented with <code>tf.slice</code> function because, for some reason, normal python indexing produced errors during TF graph compilation for me.</p>
<p>All our crucial pieces are in place, time to build the final model! We will build a subclass style model:</p>
<pre><code>class Model(tfrs.models.Model):
    def __init__(self, ...)
        ...
    def call(self, inputs):
        ...
    def compute_loss(self, inputs, training=False):
        ...
</code></pre>
<p>If we put all our layers code in a model, we get a few benefits, such as:</p>
<ul>
<li>The ability to call model.fit(dataset) method instead of writing a custom training loop with ineffective for loop in Python</li>
<li>Ability to save and load model weights</li>
<li>Ability to use &quot;task&quot; API that lets you easily add trackable metrics to your training, such as avg loss, AUC, prediction distribution etc<br />
For a subclass-style model, we have to:</li>
</ul>
<ol>
<li>Inherit from one of <a href="https://www.tensorflow.org/api_docs/python/tf/keras/Model">tf.keras.Model</a> or <a href="https://www.tensorflow.org/recommenders/api_docs/python/tfrs/models/Model">tfrs.models.Model</a>. Second one is from <code>tensorflow_recommenders</code> which is recommended for users of TPU Embedding Layer.</li>
<li>Define a <code>call</code> method. When training, input pipeline is supposed to output tuples of (features, targets) objects. <code>call()</code> method will get the first element of this tuple, i.e., features. The object can be anything, but here it is a dictionary of {feature_name: tensor}. This method is used to get predictions for the model from the given input data</li>
<li>Define a <code>compute_loss</code> method. Unlike <code>call</code>, this method gets a tuple of (features, targets) as an input. Failure to realize that will cause your pipeline to fail with some weird errors, like &quot;no gradient was provided for ...&quot;. <code>compute_loss</code> should output a scalar loss that will be used in minimization and gradient computing.
Here is how the final code will look for us:</li>
</ol>
<pre><code>class Model(tfrs.models.Model):
    def __init__(self):
        super().__init__()
        self.task = tfrs.tasks.Ranking(
            loss=tf.keras.losses.BinaryCrossentropy(
                reduction=tf.keras.losses.Reduction.NONE
            ),
            metrics=[
                tf.keras.metrics.BinaryCrossentropy(name=&quot;label-crossentropy&quot;),
                tf.keras.metrics.AUC(name=&quot;auc&quot;),
                tf.keras.metrics.AUC(curve=&quot;PR&quot;, name=&quot;pr-auc&quot;),
                tf.keras.metrics.BinaryAccuracy(name=&quot;accuracy&quot;),
            ],
            prediction_metrics=[
                tf.keras.metrics.Mean(&quot;prediction_mean&quot;),
            ],
            label_metrics=[
                tf.keras.metrics.Mean(&quot;label_mean&quot;)
            ]
        )
        self.optimizer = tf.keras.optimizers.legacy.Adam(learning_rate = lr)
        self.hashing_layer = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=vocab_size)
        embedding_layer_feature_config = {
            &quot;userId&quot;: tf.tpu.experimental.embedding.FeatureConfig(
                table=tf.tpu.experimental.embedding.TableConfig(
                vocabulary_size=vocab_size,
                initializer=initializer,
                dim=embedding_dim)),
            &quot;movieId&quot;: tf.tpu.experimental.embedding.FeatureConfig(
                table=tf.tpu.experimental.embedding.TableConfig(
                vocabulary_size=vocab_size,
                initializer=initializer,
                dim=embedding_dim)),
        }
        self.embedding_layer = tfrs.layers.embedding.TPUEmbedding(
            feature_config=embedding_layer_feature_config,
            optimizer=self.optimizer)
        self.final_activation = tf.keras.layers.Activation('sigmoid')
        

    def call(self, inputs):
        hashed_inputs = {}
        for field in [&quot;userId&quot;, &quot;movieId&quot;]:
            hashed_inputs[field] = self.hashing_layer(inputs[field])
        print(&quot;Hashed inputs: &quot;, hashed_inputs)
        embeddings = self.embedding_layer(hashed_inputs)
        user_emb = embeddings[&quot;userId&quot;]
        movie_emb = embeddings[&quot;movieId&quot;]
        # last unit of embedding is considered to be bias
        # out = tf.keras.backend.batch_dot(user_final[:, :-1], post_final[:, :-1]) + user_final[:, -1:] +  post_final[:, -1:]
        # This tf.slice code helps get read of &quot;WARNING:tensorflow:AutoGraph could not transform ...&quot; warnings produced by the above line
        # doesn't seem to improve speed though
        user_final_emb = tf.slice(user_emb, begin=[0, 0], size=[user_emb.shape[0],  user_emb.shape[1] - 1])
        user_final_bias = tf.slice(user_emb, begin=[0, user_emb.shape[1] - 1], size=[user_emb.shape[0],  1])
        movie_final_emb = tf.slice(movie_emb, begin=[0, 0], size=[movie_emb.shape[0],  movie_emb.shape[1] - 1])
        movie_final_bias = tf.slice(movie_emb, begin=[0, movie_emb.shape[1] - 1], size=[movie_emb.shape[0],  1])
        out = tf.keras.backend.batch_dot(user_final_emb, movie_final_emb) + user_final_bias + movie_final_bias
        prediction = self.final_activation(out) 
        return {
            &quot;label&quot;: prediction
        }
    def compute_loss(self, inputs, training=False):
        features, labels = inputs
        outputs = self(features, training=training)
        # loss = tf.reduce_mean(label_loss)
        loss = self.task(labels=labels[&quot;label&quot;], predictions=outputs[&quot;label&quot;])
        print(loss)
        loss = tf.reduce_mean(loss)
        return loss
</code></pre>
<p>New stuff here:</p>
<ul>
<li><code>self.task = tfrs.tasks.Ranking(...)</code> - this convenience field lets us define loss as well as a few extra metric to be tracked during training and evaluation</li>
<li><code>self.final_activation = tf.keras.layers.Activation('sigmoid')</code> - we add activation function after dot product
Now to finally do some training:</li>
</ul>
<pre><code>strategy = tf.distribute.get_strategy()
with strategy.scope():
    model = Model()
    model.compile(model.optimizer, steps_per_execution=10)
    model.fit(dataset, epochs=1, steps_per_epoch=1000)
</code></pre>
<pre><code>Tensor(&quot;while/ranking_3/Identity:0&quot;, shape=(), dtype=float32)
1000/1000 [==============================] - 2s 2ms/step - label-crossentropy: -0.9844 - auc: 0.0000e+00 - pr-auc: 1.0000 - accuracy: 0.0000e+00 - prediction_mean: 0.7430 - label_mean: 2.0000 - loss: -1.0230 - regularization_loss: 0.0000e+00 - total_loss: -1.0230
</code></pre>
<p>Here, we initialize a &quot;default strategy&quot; that is just a layer of compatibility for when we start distributed/TPU training. We compile the model, turning our python code for calling the model and computing loss into proper optimized TF graphs. <code>steps_per_execution</code> option allows you to roll out several training loop iterations into just one graph execution, which might speed it up in some weird cases.</p>
<p>This should train our model for a little bit, outputting our loss as well as some extra metrics, as promised.</p>
<p>This was a good start, we can already train a model, start experimenting, see some metrics. In the next chapter, we are going to tidy up the code a little bit.</p>
<p>As usual, code from this chapter is available as <a href="https://github.com/mgaiduk/mgaiduk.github.io/blob/main/my-first-book/src/code/chapter4/model_training.ipynb">jupyter notebook</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="chapter-5-tidying-up"><a class="header" href="#chapter-5-tidying-up">Chapter 5. Tidying up</a></h1>
<p>In the last chapter, we've written a collaborative filtering models with embedding lookup layer and dot product. We tried training it on cpu with a little bit of data.<br />
Before we scale that up to big datasets, tpus, and do some serious experimenting, we have some stuff in our code that we need to clear out:</p>
<ul>
<li>We have all our code in one file - jupyter notebook. We want to split it into several modules - model code, dataset code; a proper training script that can be run from command line; helper jupyter notebook that could access dataset and model modules - this is useful for development, when you want to actually load a bit of data, instantiate the model, look into inputs/outputs and try new stuff out interactively</li>
<li>We have a lot of constants in our code. We want to move it to command-line arguments and/or config. This include training/input pipeline options (paths, iterations, batch_size etc), input feature disposition (it is likely that at some point in the future we will be training models with different fields), model configuration (more layers, embedding sizes, vocabulary sizes)</li>
<li>We need to save the model, as well as some intermediate metrics, learning history, run parameters and stuff like that
Let s go ahead and fix them!</li>
</ul>
<h2 id="code-configuration-config-and-command-line"><a class="header" href="#code-configuration-config-and-command-line">Code configuration: config and command line</a></h2>
<p>There is no one answer to which one is better - config or command line. Most of my career I've used a c++ tool that parsed all input options for command line. Command line arguments make it easy to modify runs and experiments; configs allow more structured, nested definitions; configs are usually commited to Github, which helps make experiments more reproducible.<br />
Here, we will be using configs. I've opted for the <code>yaml</code> format, it is quite clean and easy to read and write. This is how it is supposed to work:</p>
<pre><code>%%writefile test_config.yaml
batch_size: 1024
dataset_features:
    label:
        type: &quot;int&quot;
    userId:
        type: &quot;int&quot;
    movieId:
        type: &quot;float&quot;
    timestamp:
        type: &quot;int&quot;
</code></pre>
<pre><code>import yaml
config = yaml.safe_load(open(&quot;test_config.yaml&quot;, &quot;r&quot;))
config
</code></pre>
<pre><code>{'batch_size': 1024,
 'dataset_features': {'label': {'type': 'int'},
  'userId': {'type': 'int'},
  'movieId': {'type': 'float'},
  'timestamp': {'type': 'int'}}}

</code></pre>
<p><code>yaml</code> lib loads an arbitrary yaml file into a nested python structure with dicts/lists.<br />
Here is how the full config shoud look like:</p>
<pre><code>%%writefile config.yaml
epochs: 2
global_batch_size: 8192
shuffle_buffer: 10
train_rows: 65536
trainval_rows: 8192
eval_rows: 65536
compression: &quot;GZIP&quot;
train_path: &quot;gs://mgaiduk-us-central1/ratings/csv_gzip/part*&quot;
validate_path: &quot;gs://mgaiduk-us-central1/ratings_validate/csv_gzip/part*&quot;
save_model_path: &quot;gs://mgaiduk-us-central1/models/model1&quot;
cycle_length: 8
dataset_features:
    userId:
        type: &quot;string&quot;
    movieId:
        type: &quot;string&quot;
    label:
        type: &quot;int&quot;
    timestamp:
        type: &quot;int&quot;
label: label
model:
    learning_rate: 0.01
    features:
        userId:
            hash: true
            vocab_size: 25000000
            embedding_dim: 16
            belongs_to: user
        movieId:
            hash: true
            vocab_size: 5000000
            embedding_dim: 16
            belongs_to: movie
</code></pre>
<p>Note that here, we describe the features twice: once to properly parse the input, once - to specify the model. This sounds redundant, but might be handy when you are running different models on the same dataset, and not all of the models use all of the features.<br />
We might have more features in the future (like user country, movie genre, etc), but the final layer of the model will still be a dot product between two embeddings. Extra features will have to belong either to the user part of the model, or to the movie part. This architecture is called &quot;late binding&quot; and is useful for future use at runtime: there are data structures that allow doing fast search for K Nearest Neighbors (KNN) in such a case. Examples are HNSW, SCANN. Typically, movie embeddings are baked into the index, while user embeddings are fetched on-the-fly.</p>
<p>I'd rather use some proper class fields than dictionary keys, so I wrote a parser for this config:</p>
<pre><code>%%writefile config.py
import yaml

class DatasetFeature:
    def __init__(self, feature_name, dic):
        self.name = feature_name
        self.type = dic[&quot;type&quot;]
    
    def __repr__(self):
        return &quot;DatasetFeature: &quot; + str(self.__dict__)

class Feature:
    def __init__(self, feature_name, dic):
        self.hash = False
        if &quot;hash&quot; in dic:
            self.hash = dic[&quot;hash&quot;]
            if self.hash:
                assert &quot;vocab_size&quot; in dic
                self.vocab_size = dic[&quot;vocab_size&quot;]
        self.embedding_dim = dic[&quot;embedding_dim&quot;]
        self.name = feature_name
        self.belongs_to = dic[&quot;belongs_to&quot;]

    def __repr__(self):
        return &quot;Feature: &quot; + str(self.__dict__)
        
class Model:
    def __init__(self, dic):
        self.learning_rate = dic[&quot;learning_rate&quot;]
        self.features = []
        for feature_name, feature_dic in dic[&quot;features&quot;].items():
            self.features.append(Feature(feature_name, feature_dic))

    def __repr__(self):
        return &quot;Model: &quot; + str(self.__dict__)

class Config:
    def __init__(self, path):
        dic = yaml.safe_load(open(path, 'r'))
        self.epochs = dic[&quot;epochs&quot;]
        self.compression = dic[&quot;compression&quot;]
        self.global_batch_size = dic[&quot;global_batch_size&quot;]
        self.label = dic[&quot;label&quot;]
        self.shuffle_buffer = dic[&quot;shuffle_buffer&quot;]
        self.train_path = dic[&quot;train_path&quot;]
        self.validate_path = dic[&quot;validate_path&quot;]
        self.save_model_path = dic[&quot;save_model_path&quot;]
        self.train_rows = dic[&quot;train_rows&quot;]
        self.trainval_rows = dic[&quot;trainval_rows&quot;]
        self.eval_rows = dic[&quot;eval_rows&quot;]
        self.model = Model(dic[&quot;model&quot;])
        self.dataset_features = []
        self.cycle_length = dic[&quot;cycle_length&quot;]
        for feature_name, feature_dic in dic[&quot;dataset_features&quot;].items():
            self.dataset_features.append(DatasetFeature(feature_name, feature_dic))
       
    def __repr__(self):
        return &quot;Config: &quot; + str(self.__dict__)
</code></pre>
<p>Nothing special is happening here. Here is how we can use this code:</p>
<pre><code>from config import Config
config = Config(&quot;config.yaml&quot;)
config
</code></pre>
<p>We move dataset parsing code to a separate file:</p>
<pre><code>%%writefile dataset.py
import tensorflow as tf

class DatasetReader:
    def __init__(self, config, path):
        self.config = config
        self.path = path
        defaults = []
        for feature in self.config.dataset_features:
            if feature.type  == &quot;int&quot;:
                defaults.append(tf.constant(0, dtype=tf.int64))
            elif feature.type == &quot;float&quot;:
                defaults.append(tf.constant(0.0, dtype=tf.float32))
            elif feature.type == &quot;string&quot;:
                defaults.append(tf.constant(&quot;&quot;, dtype=tf.string))
            else:
                assert False
        self.defaults = defaults

    def __call__(self, ctx: tf.distribute.InputContext):
        batch_size = ctx.get_per_replica_batch_size(
            self.config.global_batch_size) if ctx else self.config.global_batch_size
        @tf.function
        def decode_fn(record_bytes):
            csv_row = tf.io.decode_csv(record_bytes, self.defaults)
            parsed_features = {}
            for i, feature in enumerate(self.config.dataset_features):
                parsed_features[feature.name] = csv_row[i]
            features = {}
            for feature in self.config.model.features:
                t = parsed_features[feature.name]
                features[feature.name] = t
            labels = {
                &quot;label&quot;: parsed_features[self.config.label]
            }
            return (features, labels)

        def make_dataset_fn(path):
            dataset = tf.data.TextLineDataset([path], compression_type=self.config.compression.upper())
            dataset = dataset\
                .shuffle(self.config.shuffle_buffer)\
                .batch(batch_size, drop_remainder=True)\
                .repeat(self.config.epochs).map(decode_fn)
            return dataset
        filenames = tf.data.Dataset.list_files(self.path, shuffle=True, seed=42)
        if ctx and ctx.num_input_pipelines &gt; 1:
            filenames = filenames.shard(ctx.num_input_pipelines, ctx.input_pipeline_id)
        dataset = filenames.interleave(make_dataset_fn, num_parallel_calls=10, deterministic=False, cycle_length=10)
        dataset = dataset.prefetch(100)
        return dataset

def create_dataset(config, strategy, path):
    dataset_callable = DatasetReader(
        config=config,
        path=path
    )
    dataset = strategy.distribute_datasets_from_function(
        dataset_fn=dataset_callable,
        options=tf.distribute.InputOptions(experimental_fetch_to_device=False),
    )
    return dataset
</code></pre>
<p>We rewrote the code to use parameters from config instead of hard-coded ones, which includes expected defaults for csv parser.</p>
<p>We also turned our dataset code into a callable struct - this is a requirement to be able to use it in a distributed strategy.<br />
This callable is expecting to be called with the following signature:<br />
<code>def __call__(self, ctx: tf.distribute.InputContext):</code><br />
<code>ctx</code> will be passed to the call, in case we launch the training in a distributed context. If we do not, there just will be a <code>None</code>, but our code should work still.</p>
<p><code>batch_size = ctx.get_per_replica_batch_size(...)</code>: in multihost TPU or GPU training under data parallelism, the distribution disposition will look like this: there will be N workers, each reading its own portion of the dataset. One worker reads from the dataset in batches with size <code>batch_size</code>, and gives it to one of its K  devices (TPU cores or GPU devices). The devices then store that batch in the on-device memory, process it independently, but accumulate gradients in sync. Thus, the effective batch size (which might affect training quality, not just speed) will be batch_size * K = global_batch_size. Conveniently, the context provides the function to calculate batch size from global_batch_size, since the information about total device count available to this worker is stored in this context.</p>
<pre><code>if ctx and ctx.num_input_pipelines &gt; 1:
    filenames = filenames.shard(ctx.num_input_pipelines, ctx.input_pipeline_id)
</code></pre>
<p>Here we do the sharding, again, knowing the total number of workers and current worker id.</p>
<p><code>dataset = strategy.distribute_datasets_from_function(...)</code> - this call is needed to announce that instead of reading the dataset locally, on the machine that is launching the training, we will be executing this code remotely on multiple workers.
<code>options=tf.distribute.InputOptions(experimental_fetch_to_device=False)</code>: this fixes the error <code>ValueError: Received input tensor postId which is on a TPU input device /job:worker/replica:0/task:0/device:TPU:0. Input tensors for TPU embeddings must be placed on the CPU. Please ensure that your dataset is prefetching tensors to the host by setting the 'experimental_prefetch_to_device' option of the dataset distribution function. See the documentation of the enqueue method for an example.</code>. I dont quite know what it means.</p>
<p>Let's test this dataset code:</p>
<pre><code>import tensorflow as tf
from dataset import create_dataset
strategy = tf.distribute.get_strategy()
dataset = create_dataset(config, strategy, config.train_path)
for elem in dataset:
    break
elem
</code></pre>
<p>Our code is getting cleaner! Creating a dataset now takes just 2 lines of code, provided that we use the module that we created earlier. And we can still use it with a local strategy, iterate over the elements, print them out, try to apply some layers to them.</p>
<h2 id="the-model"><a class="header" href="#the-model">The model</a></h2>
<pre><code>%%writefile model.py
import math
import tensorflow as tf
import tensorflow_recommenders as tfrs

class BaseModel(tfrs.models.Model):
    def __init__(self):
        super().__init__()
        self.task = tfrs.tasks.Ranking(
            loss=tf.keras.losses.BinaryCrossentropy(
                reduction=tf.keras.losses.Reduction.NONE
            ),
            metrics=[
                tf.keras.metrics.BinaryCrossentropy(name=&quot;label-crossentropy&quot;),
                tf.keras.metrics.AUC(name=&quot;auc&quot;),
                tf.keras.metrics.AUC(curve=&quot;PR&quot;, name=&quot;pr-auc&quot;),
                tf.keras.metrics.BinaryAccuracy(name=&quot;accuracy&quot;),
            ],
            prediction_metrics=[
                tf.keras.metrics.Mean(&quot;prediction_mean&quot;),
            ],
            label_metrics=[
                tf.keras.metrics.Mean(&quot;label_mean&quot;)
            ]
        )
    
    def call(self, inputs):
        raise NotImplementedError

    def compute_loss(self, inputs, training=False):
        features, labels = inputs
        outputs = self(features, training=training)
        # loss = tf.reduce_mean(label_loss)
        loss = self.task(labels=labels[&quot;label&quot;], predictions=outputs[&quot;label&quot;])
        loss = tf.reduce_mean(loss)
        return loss

class Model(BaseModel):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.embedding_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate = config.model.learning_rate)
        self.optimizer = tf.keras.optimizers.Adam(learning_rate = config.model.learning_rate)
        self.hashing_layers = {}
        embedding_layer_feature_config = {}
        for feature in self.config.model.features:
            if feature.hash:
                self.hashing_layers[feature.name] = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=feature.vocab_size)
            initializer = tf.initializers.TruncatedNormal(
                mean=0.0, stddev=1 / math.sqrt(feature.embedding_dim)
            )
            embedding_layer_feature_config[feature.name] = tf.tpu.experimental.embedding.FeatureConfig(
                table=tf.tpu.experimental.embedding.TableConfig(
                vocabulary_size=feature.vocab_size,
                initializer=initializer,
                dim=feature.embedding_dim))
        self.embedding_layer = tfrs.layers.embedding.TPUEmbedding(
            feature_config=embedding_layer_feature_config,
            optimizer=self.embedding_optimizer)
        self.final_activation = tf.keras.layers.Activation('sigmoid')
        

    def call(self, inputs):
        features = {}
        for feature in self.config.model.features:
            t = inputs[feature.name]
            if feature.hash:
                t = self.hashing_layers[feature.name](t)
            features[feature.name] = t
        embeddings = self.embedding_layer(features)
        user_embs = []
        movie_embs = []
        for feature in self.config.model.features:
            embedding = embeddings[feature.name]
            if feature.belongs_to == &quot;user&quot;:
                user_embs.append(embedding)
            elif feature.belongs_to == &quot;movie&quot;:
                movie_embs.append(embedding)
            else:
                assert False
        user_final = tf.concat(user_embs, axis = 1)
        movie_final = tf.concat(movie_embs, axis = 1)
        # last unit of embedding is considered to be bias
        # out = tf.keras.backend.batch_dot(user_final[:, :-1], post_final[:, :-1]) + user_final[:, -1:] +  post_final[:, -1:]
        # This tf.slice code helps get read of &quot;WARNING:tensorflow:AutoGraph could not transform ...&quot; warnings produced by the above line
        # doesn't seem to improve speed though
        # user_final_emb = tf.slice(user_final, begin=[0, 0], size=[user_final.shape[0],  user_final.shape[1] - 1])
        # user_final_bias = tf.slice(user_final, begin=[0, user_final.shape[1] - 1], size=[user_final.shape[0],  1])
        # movie_final_emb = tf.slice(movie_final, begin=[0, 0], size=[movie_final.shape[0],  movie_final.shape[1] - 1])
        # movie_final_bias = tf.slice(movie_final, begin=[0, movie_final.shape[1] - 1], size=[movie_final.shape[0],  1])
        user_final_emb = user_final[:,:-1]
        user_final_bias = user_final[:,-1:]
        movie_final_emb = movie_final[:,:-1]
        movie_final_bias = movie_final[:,-1:]
        out = tf.keras.backend.batch_dot(user_final_emb, movie_final_emb) + user_final_bias + movie_final_bias
        prediction = self.final_activation(out) 
        return {
            &quot;label&quot;: prediction
        }
</code></pre>
<p>Nothing particularly interesting happening here - we just rewrote the code to use the config for parameters/feature names and stuff like that.</p>
<p>Here is how we can construct the model now:</p>
<pre><code>from model import Model
with strategy.scope():
    model = Model(config)
    model.compile(model.optimizer, steps_per_execution=10)
model(elem[0]) # see some model predictions
</code></pre>
<h2 id="the-traning-loop"><a class="header" href="#the-traning-loop">The traning loop</a></h2>
<pre><code>import sys
import os
def save_string_gcs(string_object, gcs_dir, filename):
    string_string = json.dumps(string_object)
    with open(filename, &quot;w&quot;) as f:
        f.write(string_string)
    os.system(f&quot;gsutil -m cp {filename} {gcs_dir}/{filename}&quot;)
    os.system(f&quot;rm {filename}&quot;)

train_dataset = create_dataset(config, strategy, config.train_path)
trainval_dataset = create_dataset(config, strategy, config.validate_path)
eval_dataset = create_dataset(config, strategy, config.validate_path)
train_steps_per_epoch = config.train_rows // config.global_batch_size
trainval_steps_per_epoch = config.trainval_rows // config.global_batch_size
eval_steps_per_epoch = config.eval_rows // config.global_batch_size
checkpoints_cb = tf.keras.callbacks.ModelCheckpoint(config.save_model_path  + '/checkpoints/',  save_freq = train_steps_per_epoch//3)
callbacks=[checkpoints_cb]
history = model.fit(train_dataset, epochs=config.epochs, callbacks=callbacks, steps_per_epoch=train_steps_per_epoch,
validation_data=trainval_dataset, validation_steps=trainval_steps_per_epoch)
model.save_weights(config.save_model_path  + '/weights/')
eval_steps = config.eval_rows // config.global_batch_size
eval_scores = model.evaluate(eval_dataset, return_dict=True, steps=eval_steps_per_epoch)
metrics = {}
metrics[&quot;eval&quot;] = eval_scores
metrics[&quot;history&quot;] = history.history
metrics[&quot;args&quot;] = sys.argv
metrics[&quot;config&quot;] = repr(config)
save_string_gcs(json.dumps(metrics), config.save_model_path, f&quot;metrics_pretrain.json&quot;)
</code></pre>
<p>There are some interesting things here.<br />
<code>train_steps_per_epoch = config.train_rows // config.global_batch_size</code>: as is typical for neural network training, training is split into several epochs. Training loop does some extra evaluations at the end of every epochs. In our dataset code, we assume that every epoch reads the entire dataset (hence <code>dataset.repeat(epochs)</code>), but that is not necessary. Important thing is that our dataset is limited in size, and that size is not known to the training loop. We have to provide it externally and to do it correcly, otherwise training will fail with &quot;dataset exhausted&quot; error. Steps are measured in batches, so we have to know our original data size (can check it in Bigquery) and divide it by batch size.</p>
<p><code>checkpoints_cb = tf.keras.callbacks.ModelCheckpoint</code> - this is the callback that will save model weights during the training. We can provide <code>gs://</code> path as directory to save model to.</p>
<p><code>model.save_weights()</code> - save final model after the tranining</p>
<p><code>eval_scores = model.evaluate()</code> - do final evaluation on full eval dataset (evaluation on part of the dataset was done during training)</p>
<p><code>save_string_gcs()</code> - we save training history, final evaluation results, config and run arguments to gcs as well for reproducibility.</p>
<h2 id="conclusion"><a class="header" href="#conclusion">Conclusion</a></h2>
<p>By now, we have a complete, neat, configurable code to train our model on CPU on one worker. Next, we will discuss how to launch tranining on TPUs.</p>
<p>As usual, code for this chapter is available as <a href="https://github.com/mgaiduk/mgaiduk.github.io/blob/main/my-first-book/src/code/chapter5/tidying_up.ipynb">jupyter notebook</a>. You should also checkout the <a href="https://github.com/mgaiduk/mgaiduk.github.io/blob/main/my-first-book/src/code/chapter5/playground.ipynb">jupyter notebook playground</a> that allows you to load dataset and models from the module files that we wrote, and <a href="https://github.com/mgaiduk/mgaiduk.github.io/blob/main/my-first-book/src/code/chapter5/train.py">train script</a> that you can launch from command line to train the model.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="chapter-6-training-on-tpu"><a class="header" href="#chapter-6-training-on-tpu">Chapter 6. Training on TPU</a></h1>
<h2 id="what-are-tpus"><a class="header" href="#what-are-tpus">What are TPUs?</a></h2>
<p>TPUs are ASICs from Google designed specifically for neural networks training. Processing cores are connected in such a way as to allow typical model training workflows, like matrix multiplication, to be done quickly without intermediate memory reads and writes. In addition, worker node was designed with typical training task in mind, and so it has A LOT OF MEMORY (which is a big problem for GPUs).</p>
<p>In addition, TPUs are very expensive. They do allow you to scale up your training in some cases, like deep neural network training. In other cases, they are powerful but not cost-effective. One such case happens to be collaborative filtering - the model itself is quite shallow in such a case, training speed is bounded by input pipeline throughput rather then actual training compute. It is, however, handy to be able to quickly try them out and do the experiment yourself.</p>
<p>Luckily, we wrote our code in such a way as to make it easy to apply on TPUs.</p>
<p>First, do some modifications to our training code:</p>
<pre><code># train.py
parser = argparse.ArgumentParser()
parser.add_argument(&quot;-c&quot;, &quot;--config&quot;, type=str, required=True, help=&quot;Path to config&quot;)
parser.add_argument(&quot;--tpu_name&quot;, type=str)
parser.add_argument(&quot;--tpu_zone&quot;, type=str)
args = parser.parse_args()
config = Config(args.config)
if args.tpu_name:
    assert args.tpu_zone
    handle = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=args.tpu_name, zone=args.tpu_zone)
    tpu = handle.connect(args.tpu_name, args.tpu_zone)
    strategy = tf.distribute.TPUStrategy(tpu)
    tf.tpu.experimental.initialize_tpu_system(handle)
    print(&quot;num_replicas_in_sync: &quot;, strategy.num_replicas_in_sync)
else:
    strategy = tf.distribute.get_strategy()
</code></pre>
<p>We read tpu_name and tpu_zone from command line arguments, initialize a cluster resolver - Python object that tries to communicate to TPUs over network. Then we initialize our distributed TPUStrategy, and run our training code as normal.</p>
<p>I also had to get rid of <code>tf.keras.layers.experimental.preprocessing.Hashing</code> layer in the model, replacing it with <code>tf.strings.to_hash_bucket</code> step in dataset preprocessing instead, because I was getting the following error:</p>
<pre><code>ValueError: Received input tensor postId which is the output of op model/hashing_1/hash (type StringToHashBucketFast) which does not have the `_tpu_input_identity` attr. Please ensure that the inputs to this layer are taken directly from the arguments of the function called by strategy.run. Two possible causes are: dynamic batch size support or you are using a keras layer and are not passing tensors which match the dtype of the `tf.keras.Input`s.If you are triggering dynamic batch size support, you can disable it by passing tf.distribute.RunOptions(experimental_enable_dynamic_batch_size=False) to the options argument of strategy.run().
</code></pre>
<p>Now, run this code to create a tpu:</p>
<pre><code>export TPU_ZONE=europe-west4-a
# WARNING! Billable and expensive
&gt; yes | gcloud beta compute tpus create mgaiduk_recall1 --zone=$TPU_ZONE --version=2.11 --accelerator-type=v3-8 --project=my-first-project;
# Run this after the training to make sure you delete your TPU and stop receiving bills for it
#&gt; yes | gcloud compute tpus delete mgaiduk_recall1 --zone=$TPU_ZONE --project=my-first-project|| true
</code></pre>
<p>This might require some additional setup - registering, quota handling and stuff like that, which I will not describe here.</p>
<p>Finally, run our training code:</p>
<pre><code>python3 train.py --tpu_name mgaiduk_recall1 --tpu_zone $TPU_ZONE -c config.yaml
</code></pre>
<p>And don't forget to shut down the tpu afterwards:</p>
<pre><code>gcloud compute tpus delete mgaiduk_recall1 --zone=$TPU_ZONE --project=maximal-furnace-783
</code></pre>
<p>And that's it! Training code with all necessary modifications is available at <a href="https://github.com/mgaiduk/mgaiduk.github.io/tree/main/my-first-book/src/code/chapter6">github</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="chapter-7-lazyadam-or-some-hacks-for-training-speed"><a class="header" href="#chapter-7-lazyadam-or-some-hacks-for-training-speed">Chapter 7. LazyAdam, or some hacks for training speed</a></h1>
<p>LazyAdam proved out to be so important, that it deserves its own chapter!</p>
<h2 id="what-is-adam-and-why-is-it-slow"><a class="header" href="#what-is-adam-and-why-is-it-slow">What is Adam and why is it slow?</a></h2>
<p>Adam is an optimizer that keeps track of exponentially decaying 1- and 2-order moments of all the gradients. These moments (mean and variance) are used for updating the parameters on each step, instead of just the pure &quot;gradient&quot; computed at current step in classical Gradient Descent. This feature is supposed to help in batch-training setup which is the case for pretty much all model tranining in the world - when the model only sees a small part of the data on each step, and tries to approximate the global loss function.</p>
<p>The problem with that is if you have a large embedding table, say, as we did for users - 25 million embeddings, only one embedding participating in prediction for one input record, naive Adam implementation will still update those exponentially decaying moments for ALL embeddings in the matrix. This makes it thousands of times slower than some other algorithms (like adagrad), and dependant on embedding size.</p>
<p>This problem is partially mitigated by enormous batch sizes. In previous chapters, we had a batch size of 500k. This helps the model to &quot;touch&quot; more embeddings during one batch, partially mitigating the issue of updating momentums for untouched embeddings. In practice, increasing batch size with this naive Adam implementation helps speed up the training, especially on TPU, but might introduce other problems, for example with memory or convergence.</p>
<h2 id="introducing-lazy-adam"><a class="header" href="#introducing-lazy-adam">Introducing: lazy adam!</a></h2>
<p>Here is the relevant discussion: https://groups.google.com/a/tensorflow.org/g/discuss/c/6GvpMz8kb-U/m/FaBAJkbvEQAJ</p>
<p>LazyAdam is an alternative implementation of Adam that updates momentums only for embeddings actually touched in current batch. It is available in two flavors: as an optimizer in tensorflow_addons package: <a href="https://www.tensorflow.org/addons/tutorials/optimizers_lazyadam">LazyAdam</a>, and as an option for TPU optimizer <a href="https://www.tensorflow.org/api_docs/python/tf/tpu/experimental/embedding/Adam">tf.tpu.experimental.embedding.Adam</a> in later versions of TF (&gt;= 2.11.0). In my case, speed up was up to 10x on cpu (or 1000x on smaller batches), and about 2-2.5x on TPU.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
