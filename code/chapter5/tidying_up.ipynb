{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c715e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_config.yaml\n",
    "batch_size: 1024\n",
    "dataset_features:\n",
    "    label:\n",
    "        type: \"int\"\n",
    "    userId:\n",
    "        type: \"int\"\n",
    "    movieId:\n",
    "        type: \"float\"\n",
    "    timestamp:\n",
    "        type: \"int\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf2307a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 1024,\n",
       " 'dataset_features': {'label': {'type': 'int'},\n",
       "  'userId': {'type': 'int'},\n",
       "  'movieId': {'type': 'float'},\n",
       "  'timestamp': {'type': 'int'}}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "config = yaml.safe_load(open(\"test_config.yaml\", \"r\"))\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f841e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile config.yaml\n",
    "epochs: 2\n",
    "global_batch_size: 8192\n",
    "shuffle_buffer: 10\n",
    "train_rows: 65536\n",
    "trainval_rows: 8192\n",
    "eval_rows: 65536\n",
    "compression: \"GZIP\"\n",
    "train_path: \"gs://mgaiduk-us-central1/ratings/csv_gzip/part*\"\n",
    "validate_path: \"gs://mgaiduk-us-central1/ratings_validate/csv_gzip/part*\"\n",
    "save_model_path: \"gs://mgaiduk-us-central1/models/model1\"\n",
    "cycle_length: 8\n",
    "dataset_features:\n",
    "    userId:\n",
    "        type: \"string\"\n",
    "    movieId:\n",
    "        type: \"string\"\n",
    "    label:\n",
    "        type: \"int\"\n",
    "    timestamp:\n",
    "        type: \"int\"\n",
    "label: label\n",
    "model:\n",
    "    learning_rate: 0.01\n",
    "    features:\n",
    "        userId:\n",
    "            hash: true\n",
    "            vocab_size: 25000000\n",
    "            embedding_dim: 16\n",
    "            belongs_to: user\n",
    "        movieId:\n",
    "            hash: true\n",
    "            vocab_size: 5000000\n",
    "            embedding_dim: 16\n",
    "            belongs_to: movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2904177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.py\n",
    "import yaml\n",
    "\n",
    "class DatasetFeature:\n",
    "    def __init__(self, feature_name, dic):\n",
    "        self.name = feature_name\n",
    "        self.type = dic[\"type\"]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"DatasetFeature: \" + str(self.__dict__)\n",
    "\n",
    "class Feature:\n",
    "    def __init__(self, feature_name, dic):\n",
    "        self.hash = False\n",
    "        if \"hash\" in dic:\n",
    "            self.hash = dic[\"hash\"]\n",
    "            if self.hash:\n",
    "                assert \"vocab_size\" in dic\n",
    "                self.vocab_size = dic[\"vocab_size\"]\n",
    "        self.embedding_dim = dic[\"embedding_dim\"]\n",
    "        self.name = feature_name\n",
    "        self.belongs_to = dic[\"belongs_to\"]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Feature: \" + str(self.__dict__)\n",
    "        \n",
    "class Model:\n",
    "    def __init__(self, dic):\n",
    "        self.learning_rate = dic[\"learning_rate\"]\n",
    "        self.features = []\n",
    "        for feature_name, feature_dic in dic[\"features\"].items():\n",
    "            self.features.append(Feature(feature_name, feature_dic))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Model: \" + str(self.__dict__)\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, path):\n",
    "        dic = yaml.safe_load(open(path, 'r'))\n",
    "        self.epochs = dic[\"epochs\"]\n",
    "        self.compression = dic[\"compression\"]\n",
    "        self.global_batch_size = dic[\"global_batch_size\"]\n",
    "        self.label = dic[\"label\"]\n",
    "        self.shuffle_buffer = dic[\"shuffle_buffer\"]\n",
    "        self.train_path = dic[\"train_path\"]\n",
    "        self.validate_path = dic[\"validate_path\"]\n",
    "        self.save_model_path = dic[\"save_model_path\"]\n",
    "        self.train_rows = dic[\"train_rows\"]\n",
    "        self.trainval_rows = dic[\"trainval_rows\"]\n",
    "        self.eval_rows = dic[\"eval_rows\"]\n",
    "        self.model = Model(dic[\"model\"])\n",
    "        self.dataset_features = []\n",
    "        self.cycle_length = dic[\"cycle_length\"]\n",
    "        for feature_name, feature_dic in dic[\"dataset_features\"].items():\n",
    "            self.dataset_features.append(DatasetFeature(feature_name, feature_dic))\n",
    "       \n",
    "    def __repr__(self):\n",
    "        return \"Config: \" + str(self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d14ebcfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config: {'epochs': 2, 'compression': 'GZIP', 'global_batch_size': 8192, 'label': 'label', 'shuffle_buffer': 10, 'train_path': 'gs://mgaiduk-us-central1/ratings/csv_gzip/part*', 'validate_path': 'gs://mgaiduk-us-central1/ratings_validate/csv_gzip/part*', 'save_model_path': 'gs://mgaiduk-us-central1/models/model1', 'train_rows': 65536, 'trainval_rows': 8192, 'eval_rows': 65536, 'model': Model: {'learning_rate': 0.01, 'features': [Feature: {'hash': True, 'vocab_size': 25000000, 'embedding_dim': 16, 'name': 'userId', 'belongs_to': 'user'}, Feature: {'hash': True, 'vocab_size': 5000000, 'embedding_dim': 16, 'name': 'movieId', 'belongs_to': 'movie'}]}, 'dataset_features': [DatasetFeature: {'name': 'userId', 'type': 'string'}, DatasetFeature: {'name': 'movieId', 'type': 'string'}, DatasetFeature: {'name': 'label', 'type': 'int'}, DatasetFeature: {'name': 'timestamp', 'type': 'int'}], 'cycle_length': 8}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from config import Config\n",
    "config = Config(\"config.yaml\")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1c3091",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dataset.py\n",
    "import tensorflow as tf\n",
    "\n",
    "class DatasetReader:\n",
    "    def __init__(self, config, path):\n",
    "        self.config = config\n",
    "        self.path = path\n",
    "        defaults = []\n",
    "        for feature in self.config.dataset_features:\n",
    "            if feature.type  == \"int\":\n",
    "                defaults.append(tf.constant(0, dtype=tf.int64))\n",
    "            elif feature.type == \"float\":\n",
    "                defaults.append(tf.constant(0.0, dtype=tf.float32))\n",
    "            elif feature.type == \"string\":\n",
    "                defaults.append(tf.constant(\"\", dtype=tf.string))\n",
    "            else:\n",
    "                assert False\n",
    "        self.defaults = defaults\n",
    "\n",
    "    def __call__(self, ctx: tf.distribute.InputContext):\n",
    "        batch_size = ctx.get_per_replica_batch_size(\n",
    "            self.config.global_batch_size) if ctx else self.config.global_batch_size\n",
    "        @tf.function\n",
    "        def decode_fn(record_bytes):\n",
    "            csv_row = tf.io.decode_csv(record_bytes, self.defaults)\n",
    "            parsed_features = {}\n",
    "            for i, feature in enumerate(self.config.dataset_features):\n",
    "                parsed_features[feature.name] = csv_row[i]\n",
    "            features = {}\n",
    "            for feature in self.config.model.features:\n",
    "                t = parsed_features[feature.name]\n",
    "                if feature.hash:\n",
    "                    t = tf.strings.to_hash_bucket(t, feature.vocab_size)\n",
    "                features[feature.name] = t\n",
    "            labels = {\n",
    "                \"label\": parsed_features[self.config.label]\n",
    "            }\n",
    "            return (features, labels)\n",
    "\n",
    "        def make_dataset_fn(path):\n",
    "            dataset = tf.data.TextLineDataset([path], compression_type=self.config.compression.upper())\n",
    "            dataset = dataset\\\n",
    "                .shuffle(self.config.shuffle_buffer)\\\n",
    "                .batch(batch_size, drop_remainder=True)\\\n",
    "                .repeat(self.config.epochs).map(decode_fn)\n",
    "            return dataset\n",
    "        filenames = tf.data.Dataset.list_files(self.path, shuffle=True, seed=42)\n",
    "        if ctx and ctx.num_input_pipelines > 1:\n",
    "            filenames = filenames.shard(ctx.num_input_pipelines, ctx.input_pipeline_id)\n",
    "        dataset = filenames.interleave(make_dataset_fn, num_parallel_calls=10, deterministic=False, cycle_length=10)\n",
    "        dataset = dataset.prefetch(100)\n",
    "        return dataset\n",
    "\n",
    "def create_dataset(config, strategy, path):\n",
    "    dataset_callable = DatasetReader(\n",
    "        config=config,\n",
    "        path=path\n",
    "    )\n",
    "    dataset = strategy.distribute_datasets_from_function(\n",
    "        dataset_fn=dataset_callable,\n",
    "        options=tf.distribute.InputOptions(experimental_fetch_to_device=False),\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a14417ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 16:54:27.140807: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-08 16:54:27.506699: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-08 16:54:27.517628: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-08 16:54:27.517640: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-08 16:54:28.691250: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-08 16:54:28.691305: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-08 16:54:28.691309: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-03-08 16:54:29.887300: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-08 16:54:29.887328: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-08 16:54:29.887343: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (mgaiduk-cpu-vm): /proc/driver/nvidia/version does not exist\n",
      "2023-03-08 16:54:29.887555: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'userId': <tf.Tensor: shape=(8192,), dtype=int64, numpy=array([ 4907422, 12027942,  9464171, ..., 14247112, 16568631, 23811830])>,\n",
       "  'movieId': <tf.Tensor: shape=(8192,), dtype=int64, numpy=array([3110186,  788626, 3951215, ..., 4347734, 3513937, 4559261])>},\n",
       " {'label': <tf.Tensor: shape=(8192,), dtype=int64, numpy=array([2, 2, 2, ..., 2, 2, 2])>})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from dataset import create_dataset\n",
    "strategy = tf.distribute.get_strategy()\n",
    "dataset = create_dataset(config, strategy, config.train_path)\n",
    "for elem in dataset:\n",
    "    break\n",
    "elem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb1f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model.py\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "class BaseModel(tfrs.models.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.task = tfrs.tasks.Ranking(\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(\n",
    "                reduction=tf.keras.losses.Reduction.NONE\n",
    "            ),\n",
    "            metrics=[\n",
    "                tf.keras.metrics.BinaryCrossentropy(name=\"label-crossentropy\"),\n",
    "                tf.keras.metrics.AUC(name=\"auc\"),\n",
    "                tf.keras.metrics.AUC(curve=\"PR\", name=\"pr-auc\"),\n",
    "                tf.keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "            ],\n",
    "            prediction_metrics=[\n",
    "                tf.keras.metrics.Mean(\"prediction_mean\"),\n",
    "            ],\n",
    "            label_metrics=[\n",
    "                tf.keras.metrics.Mean(\"label_mean\")\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def compute_loss(self, inputs, training=False):\n",
    "        features, labels = inputs\n",
    "        outputs = self(features, training=training)\n",
    "        # loss = tf.reduce_mean(label_loss)\n",
    "        loss = self.task(labels=labels[\"label\"], predictions=outputs[\"label\"])\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        return loss\n",
    "\n",
    "class Model(BaseModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate = config.model.learning_rate)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate = config.model.learning_rate)\n",
    "        self.hashing_layers = {}\n",
    "        embedding_layer_feature_config = {}\n",
    "        for feature in self.config.model.features:\n",
    "            if feature.hash:\n",
    "                self.hashing_layers[feature.name] = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=feature.vocab_size)\n",
    "            initializer = tf.initializers.TruncatedNormal(\n",
    "                mean=0.0, stddev=1 / math.sqrt(feature.embedding_dim)\n",
    "            )\n",
    "            embedding_layer_feature_config[feature.name] = tf.tpu.experimental.embedding.FeatureConfig(\n",
    "                table=tf.tpu.experimental.embedding.TableConfig(\n",
    "                vocabulary_size=feature.vocab_size,\n",
    "                initializer=initializer,\n",
    "                dim=feature.embedding_dim))\n",
    "        self.embedding_layer = tfrs.layers.embedding.TPUEmbedding(\n",
    "            feature_config=embedding_layer_feature_config,\n",
    "            optimizer=self.embedding_optimizer)\n",
    "        self.final_activation = tf.keras.layers.Activation('sigmoid')\n",
    "        \n",
    "\n",
    "    def call(self, inputs):\n",
    "        features = {}\n",
    "        for feature in self.config.model.features:\n",
    "            t = inputs[feature.name]\n",
    "            if feature.hash:\n",
    "                t = self.hashing_layers[feature.name](t)\n",
    "            features[feature.name] = t\n",
    "        embeddings = self.embedding_layer(features)\n",
    "        user_embs = []\n",
    "        movie_embs = []\n",
    "        for feature in self.config.model.features:\n",
    "            embedding = embeddings[feature.name]\n",
    "            if feature.belongs_to == \"user\":\n",
    "                user_embs.append(embedding)\n",
    "            elif feature.belongs_to == \"movie\":\n",
    "                movie_embs.append(embedding)\n",
    "            else:\n",
    "                assert False\n",
    "        user_final = tf.concat(user_embs, axis = 1)\n",
    "        movie_final = tf.concat(movie_embs, axis = 1)\n",
    "        # last unit of embedding is considered to be bias\n",
    "        # out = tf.keras.backend.batch_dot(user_final[:, :-1], post_final[:, :-1]) + user_final[:, -1:] +  post_final[:, -1:]\n",
    "        # This tf.slice code helps get read of \"WARNING:tensorflow:AutoGraph could not transform ...\" warnings produced by the above line\n",
    "        # doesn't seem to improve speed though\n",
    "        # user_final_emb = tf.slice(user_final, begin=[0, 0], size=[user_final.shape[0],  user_final.shape[1] - 1])\n",
    "        # user_final_bias = tf.slice(user_final, begin=[0, user_final.shape[1] - 1], size=[user_final.shape[0],  1])\n",
    "        # movie_final_emb = tf.slice(movie_final, begin=[0, 0], size=[movie_final.shape[0],  movie_final.shape[1] - 1])\n",
    "        # movie_final_bias = tf.slice(movie_final, begin=[0, movie_final.shape[1] - 1], size=[movie_final.shape[0],  1])\n",
    "        user_final_emb = user_final[:,:-1]\n",
    "        user_final_bias = user_final[:,-1:]\n",
    "        movie_final_emb = movie_final[:,:-1]\n",
    "        movie_final_bias = movie_final[:,-1:]\n",
    "        out = tf.keras.backend.batch_dot(user_final_emb, movie_final_emb) + user_final_bias + movie_final_bias\n",
    "        prediction = self.final_activation(out) \n",
    "        return {\n",
    "            \"label\": prediction\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1876b55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': <tf.Tensor: shape=(8192, 1), dtype=float32, numpy=\n",
       " array([[0.6423675 ],\n",
       "        [0.52241415],\n",
       "        [0.50483507],\n",
       "        ...,\n",
       "        [0.55719084],\n",
       "        [0.6364841 ],\n",
       "        [0.5118672 ]], dtype=float32)>}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import Model\n",
    "with strategy.scope():\n",
    "    model = Model(config)\n",
    "    model.compile(model.optimizer, steps_per_execution=1)\n",
    "model(elem[0]) # see some model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8419b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1/8 [==>...........................] - ETA: 21s - label-crossentropy: 0.7036 - auc: 0.0000e+00 - pr-auc: 1.0000 - accuracy: 0.0000e+00 - prediction_mean: 0.5010 - label_mean: 2.0000 - loss: 0.7036 - regularization_loss: 0.0000e+00 - total_loss: 0.7036"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) movieId, userId with unsupported characters which will be renamed to movieid, userid in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as ranking_1_layer_call_fn, ranking_1_layer_call_and_return_conditional_losses, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "def save_string_gcs(string_object, gcs_dir, filename):\n",
    "    string_string = json.dumps(string_object)\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(string_string)\n",
    "    os.system(f\"gsutil -m cp {filename} {gcs_dir}/{filename}\")\n",
    "    os.system(f\"rm {filename}\")\n",
    "\n",
    "with strategy.scope():\n",
    "    model = Model(config)\n",
    "    model.compile(model.optimizer, steps_per_execution=1)\n",
    "    train_dataset = create_dataset(config, strategy, config.train_path)\n",
    "    trainval_dataset = create_dataset(config, strategy, config.validate_path)\n",
    "    eval_dataset = create_dataset(config, strategy, config.validate_path)\n",
    "    train_steps_per_epoch = config.train_rows // config.global_batch_size\n",
    "    trainval_steps_per_epoch = config.trainval_rows // config.global_batch_size\n",
    "    eval_steps_per_epoch = config.eval_rows // config.global_batch_size\n",
    "    checkpoints_cb = tf.keras.callbacks.ModelCheckpoint(config.save_model_path  + '/checkpoints/',  save_freq = train_steps_per_epoch//3)\n",
    "    callbacks=[checkpoints_cb]\n",
    "    history = model.fit(train_dataset, epochs=config.epochs, callbacks=[callbacks], steps_per_epoch=train_steps_per_epoch,\n",
    "    validation_data=trainval_dataset, validation_steps=trainval_steps_per_epoch)\n",
    "    model.save_weights(config.save_model_path  + '/weights/')\n",
    "    eval_steps = config.eval_rows // config.global_batch_size\n",
    "    eval_scores = model.evaluate(eval_dataset, return_dict=True, steps=eval_steps_per_epoch)\n",
    "    metrics = {}\n",
    "    metrics[\"eval\"] = eval_scores\n",
    "    metrics[\"history\"] = history.history\n",
    "    metrics[\"args\"] = sys.argv\n",
    "    metrics[\"config\"] = repr(config)\n",
    "    save_string_gcs(json.dumps(metrics), config.save_model_path, f\"metrics_pretrain.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
