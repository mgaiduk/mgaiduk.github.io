{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcee404a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-07 13:46:14.943065: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-07 13:46:15.038856: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-07 13:46:15.042080: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-07 13:46:15.042115: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-07 13:46:15.596876: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-07 13:46:15.596930: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-07 13:46:15.596935: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n",
      "v0.7.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-07 13:46:16.136767: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-07 13:46:16.136799: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-07 13:46:16.136814: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (mgaiduk-cpu-vm): /proc/driver/nvidia/version does not exist\n",
      "2023-03-07 13:46:16.137064: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'userId': <tf.Tensor: shape=(16,), dtype=int64, numpy=\n",
       "  array([   5,  154,  226,  600,  689,  717,  810,  909, 1208, 1409, 1847,\n",
       "         2071, 2105, 2177, 2429, 2476])>,\n",
       "  'movieId': <tf.Tensor: shape=(16,), dtype=int64, numpy=\n",
       "  array([  191, 69526,   736,   435,  7759,  1240,  2087,  1527,   158,\n",
       "          1722,  1037,   480,  1261,  6290,    39,  4037])>},\n",
       " {'label': <tf.Tensor: shape=(16,), dtype=float32, numpy=\n",
       "  array([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
       "        dtype=float32)>})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import tensorflow_recommenders as tfrs\n",
    "print(tfrs.__version__)\n",
    "\n",
    "import math\n",
    "@tf.function\n",
    "def decode_fn(csv_line):\n",
    "    defaults = [tf.constant(0, dtype=tf.int64),\n",
    "           tf.constant(0, dtype=tf.int64),\n",
    "           tf.constant(0, dtype=tf.float32),\n",
    "           tf.constant(0, dtype=tf.int64)]\n",
    "    csv_row = tf.io.decode_csv(csv_line, defaults)\n",
    "    features = {}\n",
    "    features[\"userId\"] = csv_row[0]\n",
    "    features[\"movieId\"] = csv_row[1]\n",
    "    labels = {\n",
    "        \"label\": csv_row[2]\n",
    "    }\n",
    "    return (features, labels)\n",
    "ctx = None # will not be none in distributed strategy, see later\n",
    "def make_dataset_fn(path):\n",
    "    dataset = tf.data.TextLineDataset([path], compression_type=\"GZIP\")\n",
    "    dataset = dataset\\\n",
    "        .batch(16, drop_remainder=True)\\\n",
    "    .map(decode_fn)\n",
    "    return dataset\n",
    "filenames = tf.data.Dataset.list_files(\"gs://mgaiduk-us-central1/ratings/csv_gzip/part*\", shuffle=True, seed=42)\n",
    "if ctx and ctx.num_input_pipelines > 1:\n",
    "    filenames = filenames.shard(ctx.num_input_pipelines, ctx.input_pipeline_id)\n",
    "dataset = filenames.interleave(make_dataset_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE, deterministic=False, cycle_length=8)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "for elem in dataset:\n",
    "    break\n",
    "elem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a7b8a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(16,), dtype=int64, numpy=\n",
       "array([971, 515, 509, 521, 490, 299, 925, 674, 347,  45, 252, 902, 563,\n",
       "       215, 886, 238])>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 1000\n",
    "hashing_layer = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=vocab_size)\n",
    "hashing_layer(elem[0][\"userId\"]) # it's good that we have an eager tensor on our hands, so we can test inputs-outputs to all parts of our model, heh?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8c2c4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maksimgaiduk/.local/lib/python3.9/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'userId': <tf.Tensor: shape=(16, 8), dtype=float32, numpy=\n",
       " array([[ 0.41343534, -0.31794858,  0.01400006, -0.1614508 , -0.3027821 ,\n",
       "         -0.2894226 ,  0.24812186, -0.28222725],\n",
       "        [ 0.08887184, -0.33360675,  0.45120153,  0.6819348 , -0.08404738,\n",
       "         -0.14298265,  0.05770994, -0.5238398 ],\n",
       "        [-0.5518302 , -0.41785422,  0.19948402,  0.18964908, -0.13487418,\n",
       "         -0.2010811 ,  0.3907623 ,  0.13097371],\n",
       "        [ 0.20046298,  0.20850155, -0.09723332, -0.15830232,  0.3276058 ,\n",
       "          0.2102013 , -0.25354075,  0.07657696],\n",
       "        [-0.12120442, -0.2367408 , -0.45796743, -0.29654923, -0.276082  ,\n",
       "         -0.41840187,  0.2705518 , -0.19380979],\n",
       "        [-0.2709396 ,  0.37509307, -0.20403184, -0.04137465,  0.25251728,\n",
       "         -0.08419357,  0.2092655 , -0.19470377],\n",
       "        [ 0.06902198,  0.08367513, -0.34424788,  0.28026772, -0.06520353,\n",
       "          0.10085491, -0.39658692,  0.03365644],\n",
       "        [ 0.20106868, -0.28069216, -0.07548938,  0.23359123, -0.00303984,\n",
       "         -0.04687683,  0.1403125 , -0.47173747],\n",
       "        [ 0.22975917, -0.24328794, -0.20251478, -0.47040895, -0.44093227,\n",
       "          0.13018015,  0.05180406, -0.05310674],\n",
       "        [-0.39321986,  0.16620786,  0.33801353,  0.12882094,  0.56146455,\n",
       "         -0.35427034,  0.19798744, -0.02888264],\n",
       "        [-0.00581496, -0.34485713,  0.31232458,  0.67587364, -0.1577708 ,\n",
       "         -0.20962346, -0.36968356, -0.01707592],\n",
       "        [ 0.1197344 ,  0.25787994,  0.10971702,  0.23502515, -0.04118537,\n",
       "         -0.6711416 , -0.18456587,  0.161597  ],\n",
       "        [ 0.255639  ,  0.16541623, -0.02888527,  0.1809602 ,  0.19653061,\n",
       "          0.07384062, -0.03326958,  0.00996237],\n",
       "        [ 0.10497079,  0.06214169, -0.46885714,  0.6410996 , -0.26477516,\n",
       "          0.1383409 ,  0.02751455, -0.2916845 ],\n",
       "        [-0.10122368, -0.02272036, -0.35374397,  0.6128082 , -0.48445717,\n",
       "         -0.13178124,  0.04918678, -0.09979887],\n",
       "        [ 0.10996483,  0.04564042,  0.4260264 ,  0.55904925,  0.24606286,\n",
       "         -0.2377725 , -0.39557186,  0.07674003]], dtype=float32)>,\n",
       " 'movieId': <tf.Tensor: shape=(16, 8), dtype=float32, numpy=\n",
       " array([[ 0.5135506 , -0.13577563, -0.03743725, -0.06017395, -0.43219328,\n",
       "         -0.29620418, -0.29978895, -0.14216894],\n",
       "        [ 0.37020624, -0.40155193,  0.44291276,  0.36991534,  0.5186614 ,\n",
       "         -0.3046441 , -0.13212118,  0.03534086],\n",
       "        [-0.33900675, -0.15374391,  0.36507288, -0.23689131, -0.3768868 ,\n",
       "         -0.6441281 , -0.6001943 ,  0.2545395 ],\n",
       "        [-0.11989204, -0.257206  , -0.13081683, -0.23296472,  0.15038888,\n",
       "          0.18120863,  0.08131325,  0.1196133 ],\n",
       "        [-0.3695637 ,  0.14451092,  0.51162773, -0.14955072,  0.629557  ,\n",
       "          0.2580483 , -0.04026871, -0.51276994],\n",
       "        [-0.4376744 , -0.00110402, -0.34383103, -0.01716815, -0.4383834 ,\n",
       "          0.01120727, -0.04305076,  0.08087189],\n",
       "        [ 0.11517566, -0.12398084,  0.39007238, -0.40837386, -0.257664  ,\n",
       "         -0.24874853, -0.24941994, -0.10686897],\n",
       "        [ 0.51148957, -0.20783126,  0.5686578 , -0.2616691 ,  0.46142998,\n",
       "          0.24580204,  0.5400442 ,  0.6116717 ],\n",
       "        [ 0.5006772 ,  0.0598735 ,  0.5418825 , -0.6254327 ,  0.0564913 ,\n",
       "          0.13970399, -0.03581604, -0.08110797],\n",
       "        [ 0.01474413, -0.19083299, -0.6976943 , -0.04522938, -0.15910797,\n",
       "         -0.30199048, -0.6919643 ,  0.39948544],\n",
       "        [ 0.11517566, -0.12398084,  0.39007238, -0.40837386, -0.257664  ,\n",
       "         -0.24874853, -0.24941994, -0.10686897],\n",
       "        [-0.3356106 ,  0.03342586, -0.6936286 ,  0.15676384, -0.6565286 ,\n",
       "         -0.13843228,  0.02153863, -0.14164282],\n",
       "        [ 0.28600308,  0.1885104 ,  0.24031903, -0.13285214,  0.04809571,\n",
       "          0.12579364, -0.21496657, -0.0573195 ],\n",
       "        [-0.6614549 ,  0.17269252, -0.39555496,  0.04274457, -0.5487856 ,\n",
       "          0.07801703,  0.52440643, -0.6753446 ],\n",
       "        [-0.15688978, -0.08519351, -0.02496243, -0.5831509 ,  0.47080004,\n",
       "          0.13600442, -0.31353462,  0.29020756],\n",
       "        [ 0.10738698,  0.16649702, -0.15803333, -0.4889691 ,  0.39244452,\n",
       "          0.61818856,  0.45805624,  0.13873667]], dtype=float32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 8\n",
    "lr = 0.01\n",
    "initializer = tf.initializers.TruncatedNormal(\n",
    "    mean=0.0, stddev=1 / math.sqrt(embedding_dim)\n",
    ")\n",
    "embedding_layer_feature_config = {\n",
    "    \"userId\": tf.tpu.experimental.embedding.FeatureConfig(\n",
    "        table=tf.tpu.experimental.embedding.TableConfig(\n",
    "        vocabulary_size=vocab_size,\n",
    "        initializer=initializer,\n",
    "        dim=embedding_dim)),\n",
    "    \"movieId\": tf.tpu.experimental.embedding.FeatureConfig(\n",
    "        table=tf.tpu.experimental.embedding.TableConfig(\n",
    "        vocabulary_size=vocab_size,\n",
    "        initializer=initializer,\n",
    "        dim=embedding_dim)),\n",
    "}\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate = lr)\n",
    "embedding_layer = tfrs.layers.embedding.TPUEmbedding(\n",
    "    feature_config=embedding_layer_feature_config,\n",
    "    optimizer=optimizer)\n",
    "hashed_tensor = {\n",
    "    \"userId\": hashing_layer(elem[0][\"userId\"]),\n",
    "    \"movieId\": hashing_layer(elem[0][\"movieId\"])\n",
    "}\n",
    "embeddings = embedding_layer(hashed_tensor)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1e82081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(16, 1), dtype=float32, numpy=\n",
       "array([[-0.01751116],\n",
       "       [ 0.12280549],\n",
       "       [ 0.6105509 ],\n",
       "       [ 0.23486945],\n",
       "       [-1.1786304 ],\n",
       "       [-0.04545186],\n",
       "       [-0.23374282],\n",
       "       [ 0.25991398],\n",
       "       [ 0.14214638],\n",
       "       [-0.02791688],\n",
       "       [-0.05103706],\n",
       "       [ 0.06510222],\n",
       "       [ 0.05184928],\n",
       "       [-0.6423423 ],\n",
       "       [-0.40173113],\n",
       "       [-0.3374161 ]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_emb = embeddings[\"userId\"]\n",
    "movie_emb = embeddings[\"movieId\"]\n",
    "user_final_emb = tf.slice(user_emb, begin=[0, 0], size=[user_emb.shape[0],  user_emb.shape[1] - 1])\n",
    "user_final_bias = tf.slice(user_emb, begin=[0, user_emb.shape[1] - 1], size=[user_emb.shape[0],  1])\n",
    "movie_final_emb = tf.slice(movie_emb, begin=[0, 0], size=[movie_emb.shape[0],  movie_emb.shape[1] - 1])\n",
    "movie_final_bias = tf.slice(movie_emb, begin=[0, movie_emb.shape[1] - 1], size=[movie_emb.shape[0],  1])\n",
    "out = tf.keras.backend.batch_dot(user_final_emb, movie_final_emb) + user_final_bias + movie_final_bias\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ade9221",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tfrs.models.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.task = tfrs.tasks.Ranking(\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(\n",
    "                reduction=tf.keras.losses.Reduction.NONE\n",
    "            ),\n",
    "            metrics=[\n",
    "                tf.keras.metrics.BinaryCrossentropy(name=\"label-crossentropy\"),\n",
    "                tf.keras.metrics.AUC(name=\"auc\"),\n",
    "                tf.keras.metrics.AUC(curve=\"PR\", name=\"pr-auc\"),\n",
    "                tf.keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "            ],\n",
    "            prediction_metrics=[\n",
    "                tf.keras.metrics.Mean(\"prediction_mean\"),\n",
    "            ],\n",
    "            label_metrics=[\n",
    "                tf.keras.metrics.Mean(\"label_mean\")\n",
    "            ]\n",
    "        )\n",
    "        self.optimizer = tf.keras.optimizers.legacy.Adam(learning_rate = lr)\n",
    "        self.hashing_layer = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=vocab_size)\n",
    "        embedding_layer_feature_config = {\n",
    "            \"userId\": tf.tpu.experimental.embedding.FeatureConfig(\n",
    "                table=tf.tpu.experimental.embedding.TableConfig(\n",
    "                vocabulary_size=vocab_size,\n",
    "                initializer=initializer,\n",
    "                dim=embedding_dim)),\n",
    "            \"movieId\": tf.tpu.experimental.embedding.FeatureConfig(\n",
    "                table=tf.tpu.experimental.embedding.TableConfig(\n",
    "                vocabulary_size=vocab_size,\n",
    "                initializer=initializer,\n",
    "                dim=embedding_dim)),\n",
    "        }\n",
    "        self.embedding_layer = tfrs.layers.embedding.TPUEmbedding(\n",
    "            feature_config=embedding_layer_feature_config,\n",
    "            optimizer=self.optimizer)\n",
    "        self.final_activation = tf.keras.layers.Activation('sigmoid')\n",
    "        \n",
    "\n",
    "    def call(self, inputs):\n",
    "        hashed_inputs = {}\n",
    "        for field in [\"userId\", \"movieId\"]:\n",
    "            hashed_inputs[field] = self.hashing_layer(inputs[field])\n",
    "        print(\"Hashed inputs: \", hashed_inputs)\n",
    "        embeddings = self.embedding_layer(hashed_inputs)\n",
    "        user_emb = embeddings[\"userId\"]\n",
    "        movie_emb = embeddings[\"movieId\"]\n",
    "        # last unit of embedding is considered to be bias\n",
    "        # out = tf.keras.backend.batch_dot(user_final[:, :-1], post_final[:, :-1]) + user_final[:, -1:] +  post_final[:, -1:]\n",
    "        # This tf.slice code helps get read of \"WARNING:tensorflow:AutoGraph could not transform ...\" warnings produced by the above line\n",
    "        # doesn't seem to improve speed though\n",
    "        user_final_emb = tf.slice(user_emb, begin=[0, 0], size=[user_emb.shape[0],  user_emb.shape[1] - 1])\n",
    "        user_final_bias = tf.slice(user_emb, begin=[0, user_emb.shape[1] - 1], size=[user_emb.shape[0],  1])\n",
    "        movie_final_emb = tf.slice(movie_emb, begin=[0, 0], size=[movie_emb.shape[0],  movie_emb.shape[1] - 1])\n",
    "        movie_final_bias = tf.slice(movie_emb, begin=[0, movie_emb.shape[1] - 1], size=[movie_emb.shape[0],  1])\n",
    "        out = tf.keras.backend.batch_dot(user_final_emb, movie_final_emb) + user_final_bias + movie_final_bias\n",
    "        prediction = self.final_activation(out) \n",
    "        return {\n",
    "            \"label\": prediction\n",
    "        }\n",
    "    def compute_loss(self, inputs, training=False):\n",
    "        features, labels = inputs\n",
    "        outputs = self(features, training=training)\n",
    "        # loss = tf.reduce_mean(label_loss)\n",
    "        loss = self.task(labels=labels[\"label\"], predictions=outputs[\"label\"])\n",
    "        print(loss)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6ff506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.get_strategy()\n",
    "with strategy.scope():\n",
    "    model = Model()\n",
    "    model.compile(model.optimizer, steps_per_execution=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2977264e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hashed inputs:  {'userId': <tf.Tensor 'model/hashing_1/Identity:0' shape=(16,) dtype=int64>, 'movieId': <tf.Tensor 'model/hashing_1/Identity_1:0' shape=(16,) dtype=int64>}\n",
      "Tensor(\"ranking/Identity:0\", shape=(), dtype=float32)\n",
      "Hashed inputs:  {'userId': <tf.Tensor 'while/model/hashing_1/Identity:0' shape=(16,) dtype=int64>, 'movieId': <tf.Tensor 'while/model/hashing_1/Identity_1:0' shape=(16,) dtype=int64>}\n",
      "Tensor(\"while/ranking/Identity:0\", shape=(), dtype=float32)\n",
      "Hashed inputs:  {'userId': <tf.Tensor 'model/hashing_1/Identity:0' shape=(16,) dtype=int64>, 'movieId': <tf.Tensor 'model/hashing_1/Identity_1:0' shape=(16,) dtype=int64>}\n",
      "Tensor(\"ranking/Identity:0\", shape=(), dtype=float32)\n",
      "Hashed inputs:  {'userId': <tf.Tensor 'while/model/hashing_1/Identity:0' shape=(16,) dtype=int64>, 'movieId': <tf.Tensor 'while/model/hashing_1/Identity_1:0' shape=(16,) dtype=int64>}\n",
      "Tensor(\"while/ranking/Identity:0\", shape=(), dtype=float32)\n",
      "1000/1000 [==============================] - 2s 2ms/step - label-crossentropy: -0.7938 - auc: 0.0000e+00 - pr-auc: 1.0000 - accuracy: 0.0000e+00 - prediction_mean: 0.7239 - label_mean: 2.0000 - loss: -0.8094 - regularization_loss: 0.0000e+00 - total_loss: -0.8094\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    model.fit(dataset, epochs=1, steps_per_epoch=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
